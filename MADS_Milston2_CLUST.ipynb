{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import eikon as ek\n",
    "import sys\n",
    "#import config\n",
    "#ek.set_app_key(config.eikon_key)\n",
    "#import DatastreamDSWS as DSWS\n",
    "#ds = DSWS.Datastream(username = 'JGarden1@lidoadvisors.com', password= 'Welcome2')\n",
    "from datetime import timedelta\n",
    "import datetime\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "from arch import arch_model\n",
    "\n",
    "from tqdm import tqdm\n",
    "TOLERANCE = 1e-15\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import cufflinks as cf\n",
    "import configparser as cp\n",
    "cf.set_config_file(offline = True)\n",
    "\n",
    "\n",
    "#import warnings\n",
    "#warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "#from scipy.optimize import OptimizeWarning\n",
    "#warnings.simplefilter(action='ignore', category=OptimizeWarning)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import chart_studio\n",
    "from chart_studio.plotly import plot, iplot\n",
    "import chart_studio.plotly as py\n",
    "from PIL import Image as im\n",
    "#import Lido_funcs3 as lf\n",
    "import time\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import IPython.display\n",
    "from IPython.display import Image\n",
    "\n",
    "import chart_studio.tools as tls\n",
    "\n",
    "import plotly.io as pio\n",
    "\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "import scipy.stats as st\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler\n",
    "from scipy.stats.mstats import winsorize\n",
    "from scipy import stats\n",
    "#ek.set_timeout(60)\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def win_df(df):\n",
    "    return df.apply(using_mstats, axis=0)\n",
    "\n",
    "def using_mstats(s):\n",
    "    return winsorize(s, limits=[0.05, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###This cell sets up a list that contains dates used to call needed data from the API\n",
    "\n",
    "\n",
    "mar = []\n",
    "jun = []\n",
    "sept = []\n",
    "dec = []\n",
    "\n",
    "for i in range(2015, 2021):\n",
    "    dt_3 = datetime.date(i, 3, 15)\n",
    "    if dt_3.isoweekday() == 6:\n",
    "        dt_3 = datetime.date(i, 3, 17)\n",
    "    elif dt_3.isoweekday() == 7:\n",
    "        dt_3 = datetime.date(i, 3, 16)\n",
    "    st_dt_3 = dt_3.strftime(\"%Y-%m-%d\")\n",
    "    mar.append(st_dt_3)\n",
    "    dt_6 = datetime.date(i, 6, 15)\n",
    "    if dt_6.isoweekday() == 6:\n",
    "        dt_6 = datetime.date(i, 6, 17)\n",
    "    elif dt_6.isoweekday() == 7:\n",
    "        dt_6 = datetime.date(i, 6, 16)\n",
    "    st_dt_6 = dt_6.strftime(\"%Y-%m-%d\")\n",
    "    jun.append(st_dt_6)\n",
    "    dt_9 = datetime.date(i, 9, 15)\n",
    "    if dt_9.isoweekday() == 6:\n",
    "        dt_9 = datetime.date(i, 9, 17)\n",
    "    elif dt_9.isoweekday() == 7:\n",
    "        dt_9 = datetime.date(i, 9, 16)\n",
    "    st_dt_9 = dt_9.strftime(\"%Y-%m-%d\")\n",
    "    sept.append(st_dt_9)\n",
    "    dt_12 = datetime.date(i, 12, 15)\n",
    "    if dt_12.isoweekday() == 6:\n",
    "        dt_12 = datetime.date(i, 12, 17)\n",
    "    elif dt_12.isoweekday() == 7:\n",
    "        dt_12 = datetime.date(i, 12, 16)\n",
    "    st_dt_12 = dt_12.strftime(\"%Y-%m-%d\")\n",
    "    dec.append(st_dt_12)\n",
    "    \n",
    "\n",
    "dates = []\n",
    "for i in range(0, len(mar)):\n",
    "    dates.append(mar[i])\n",
    "    dates.append(jun[i])\n",
    "    dates.append(sept[i])\n",
    "    dates.append(dec[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_SP5X_stocks():\n",
    "    '''This function imports the Symbols (RICS) for all stocks in the SPDR S&P 500 Deposetory recipt ETF do not run w/o a subscription to refinitive.'''\n",
    "    hold = ek.get_data('SPY', fields = [ek.TR_Field('TR.ETPConstituentRIC',params={'SDate':'2021-01-07'})])[0]\n",
    "    hold = hold[hold['Constituent RIC'] != 'GOOG.OQ']\n",
    "    rics = [x for x in hold['Constituent RIC']]\n",
    "    if 'GOOG.OQ' in rics == True:\n",
    "        rics.remove('GOOG.OQ')\n",
    "    return rics\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def API_req(spx, date):\n",
    "    '''This function calls all the features for each stock at a given date, do not run w/o a subscription to refinitive. '''\n",
    "    valuation_fields = [ek.TR_Field('TR.GICSSector'),ek.TR_Field('TR.NetProfitMean', params={'Period':'FY2', 'SDate': date}), ek.TR_Field('TR.EBITDAReportedMean',  params={'Period':'FY2', 'SDate': date}),\n",
    "                        ek.TR_Field('TR.EBITMean', params = {'Period':'FY2', 'SDate': date}), ek.TR_Field('TR.TotalEquity', params = {'Period':'FY0','SDate': date}),\n",
    "                        ek.TR_Field('TR.CompanyMarketCap', params = {'SDate': date}), ek.TR_Field('TR.EV', params = {'SDate': date})]\n",
    "    valuation_dat = ek.get_data(spx, fields=valuation_fields)[0]\n",
    "    valuation_dat.to_csv('SPX_VALUATION_{}.csv'.format(date))\n",
    "    time.sleep(10)\n",
    "        \n",
    "    fundam_fields = [ek.TR_Field('TR.GICSSector'), ek.TR_Field('TR.ROICMean', params={'Period':'FY2', 'SDate': date}), ek.TR_Field('TR.WACC'), ek.TR_Field('TR.ROEMean',params={'Period':'FY2', 'SDate': date}),\n",
    "                 ek.TR_Field('TR.WACCCostofEquity', params={'SDate': date}), ek.TR_Field('TR.LTGMean', params={'SDate': date}), ek.TR_Field('TR.TtlDebtToTtlCapitalPct', params={'Period':'FY0', 'SDate': date})]\n",
    "    fundamental_dat = ek.get_data(spx, fields=fundam_fields)[0] \n",
    "    fundamental_dat.to_csv('SPX_FUNDAM_{}.csv'.format(date))\n",
    "    time.sleep(10)\n",
    "    mom_fields = [ek.TR_Field('TR.GICSSector'), ek.TR_Field('TR.TotalReturn3Mo', params={'SDate': date}), ek.TR_Field('TR.TotalReturn6Mo', params={'SDate': date}), \n",
    "                  ek.TR_Field('TR.TotalReturn52Wk', params={'SDate': date}), ek.TR_Field('TR.WACCBeta',params={'SDate': date})]\n",
    "    momentum = ek.get_data(spx, fields=mom_fields)[0]\n",
    "    spy_rets = ek.get_data('SPY', fields=mom_fields)[0]\n",
    "    momentum.to_csv('SPX_MOM_{}.csv'.format(date))\n",
    "    spy_rets.to_csv('INDEX_MOM_{}.csv'.format(date))\n",
    "    \n",
    "    return (valuation_dat, fundamental_dat, mom_fields, spy_rets)\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_stock_dat(dates):\n",
    "    '''This function creates the dataset needed for the the rest of the project, do not run with out a Refinitive eikon subscription and takes about 30 Min '''\n",
    "    spx = get_SP5X_stocks()\n",
    "    for date in dates:\n",
    "        API_req(spx, date)\n",
    "        time.sleep(5)\n",
    "    return spx\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spx = import_stock_dat(['2020-03-16'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valuation_dat(date):\n",
    "    valuation_dat = pd.read_csv('SPX_VALUATION_{}.csv'.format(date))\n",
    "    valuation_dat['Earnings_Yield'] = valuation_dat['Net Income - Mean']/valuation_dat['Company Market Cap']\n",
    "    valuation_dat['Book_Market'] = valuation_dat['Total Equity']/valuation_dat['Company Market Cap']\n",
    "    valuation_dat['EBITDA_EV'] = valuation_dat['EBITDA Reported - Mean']/valuation_dat['Enterprise Value (Daily Time Series)']\n",
    "    valuation_dat['EBIT_EV'] = valuation_dat['EBIT - Mean']/valuation_dat['Enterprise Value (Daily Time Series)']\n",
    "    \n",
    "    sectors = list(valuation_dat['GICS Sector Name'].drop_duplicates())\n",
    "    \n",
    "    valuation = pd.DataFrame()\n",
    "    for s in sectors:\n",
    "        x =  valuation_dat[valuation_dat['GICS Sector Name']==s]\n",
    "        meds = x.median()\n",
    "        x = x.fillna(meds)\n",
    "        valuation = pd.concat([valuation, x])\n",
    "    \n",
    "    \n",
    "    valuation[['Earnings_Yield', 'Book_Market', 'EBITDA_EV', 'EBIT_EV']] = win_df(valuation[['Earnings_Yield', 'Book_Market', 'EBITDA_EV', 'EBIT_EV']])\n",
    "    \n",
    "    valuation_dat_mkt = valuation\n",
    "    valuation_dat_mkt = valuation_dat.filter(['Instrument', 'GICS Sector Name', 'Earnings_Yield', 'Book_Market', 'EBITDA_EV', 'EBIT_EV'])\n",
    "    min_max = MinMaxScaler()\n",
    "    valuation_dat_mkt[['Earnings_Yield', 'Book_Market', 'EBITDA_EV', 'EBIT_EV']] = min_max.fit_transform(valuation_dat_mkt[['Earnings_Yield', 'Book_Market', 'EBITDA_EV', 'EBIT_EV']])\n",
    "    valuation_dat_mkt['Valuation Composite'] = valuation_dat_mkt.mean(axis = 1)\n",
    "    valuation_dat_mkt[['Valuation Composite']] = min_max.fit_transform(valuation_dat_mkt[['Valuation Composite']])\n",
    "    val_comp = valuation_dat_mkt['Valuation Composite']\n",
    "    valuation_dat_mkt = valuation_dat_mkt.drop('Valuation Composite', axis = 1)\n",
    "    \n",
    "    return valuation_dat_mkt, val_comp\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fundamental_dat(date):\n",
    "    fundamental_dat = pd.read_csv('SPX_FUNDAM_{}.csv'.format(date))\n",
    "    fundamental_1 = fundamental_dat[fundamental_dat['Return On Invested Capital - Mean'].isnull()]\n",
    "    fundamental_1['Spread'] = fundamental_1['Return On Equity - Mean'] - fundamental_1['WACC Cost of Equity, (%)']\n",
    "    fundamental_2 = fundamental_dat[fundamental_dat['Return On Invested Capital - Mean'].notnull()]\n",
    "    fundamental_2['Spread'] = fundamental_2['Return On Invested Capital - Mean'] - fundamental_2['Weighted Average Cost of Capital, (%)']\n",
    "    fundamental_score = pd.concat([fundamental_2, fundamental_1])\n",
    "    fundamental_score['Equity To Total Cap'] = 100 - fundamental_score['Total Debt to Total Capital, Percent']\n",
    "    \n",
    "    sectors = list(fundamental_score['GICS Sector Name'].drop_duplicates())\n",
    "    \n",
    "    fund_df= pd.DataFrame()\n",
    "    for s in sectors:\n",
    "        x =  fundamental_score[fundamental_score['GICS Sector Name']==s]\n",
    "        meds = x.median()\n",
    "        x = x.fillna(meds)\n",
    "        fund_df= pd.concat([fund_df, x])\n",
    "    \n",
    "    fund_df[['Return On Invested Capital - Mean', 'Weighted Average Cost of Capital, (%)', 'Return On Equity - Mean', \n",
    "                       'WACC Cost of Equity, (%)', 'Long Term Growth - Mean', 'Total Debt to Total Capital, Percent', 'Spread', \n",
    "                       'Equity To Total Cap']] = win_df(fund_df[['Return On Invested Capital - Mean', 'Weighted Average Cost of Capital, (%)', 'Return On Equity - Mean', \n",
    "                       'WACC Cost of Equity, (%)', 'Long Term Growth - Mean', 'Total Debt to Total Capital, Percent', 'Spread', \n",
    "                       'Equity To Total Cap']])\n",
    "    \n",
    "    min_max = MinMaxScaler()\n",
    "    fund_df[['Return On Invested Capital - Mean', 'Weighted Average Cost of Capital, (%)', 'Return On Equity - Mean', \n",
    "                       'WACC Cost of Equity, (%)', 'Long Term Growth - Mean', 'Total Debt to Total Capital, Percent', 'Spread', \n",
    "                       'Equity To Total Cap']] = min_max.fit_transform(fund_df[['Return On Invested Capital - Mean', 'Weighted Average Cost of Capital, (%)', 'Return On Equity - Mean', \n",
    "                       'WACC Cost of Equity, (%)', 'Long Term Growth - Mean', 'Total Debt to Total Capital, Percent', 'Spread', \n",
    "                       'Equity To Total Cap']])\n",
    "\n",
    "        \n",
    "    \n",
    "    fund_df['FUND_Composite'] = fund_df[['Spread', 'Long Term Growth - Mean', 'Equity To Total Cap']].sum(axis = 1)\n",
    "    min_max = MinMaxScaler()\n",
    "    fund_df[['FUND_Composite']]  = min_max.fit_transform(fund_df[['FUND_Composite']])\n",
    "    fund_comp = fund_df[['FUND_Composite']] \n",
    "    fundamental_score = fund_df.drop('FUND_Composite', axis = 1)\n",
    "    \n",
    "    return fundamental_score, fund_comp\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def momentum_dat(date):\n",
    "    \n",
    "    momentum = pd.read_csv('SPX_MOM_{}.csv'.format(date))\n",
    "    spy_rets = pd.read_csv('INDEX_MOM_{}.csv'.format(date))\n",
    "    mom_1 = momentum[momentum['Beta'].notnull()]\n",
    "    mom_2 = momentum[momentum['Beta'].isnull()]\n",
    "    mom_1['3_month'] = mom_1['3 Month Total Return'] - (spy_rets['3 Month Total Return'].iloc[0] * mom_1['Beta'])\n",
    "    mom_1['6_month'] = mom_1['6 Month Total Return'] - (spy_rets['6 Month Total Return'].iloc[0] * mom_1['Beta'])\n",
    "    mom_1['12_month'] = mom_1['52 Week Total Return'] - (spy_rets['52 Week Total Return'].iloc[0] * mom_1['Beta'])\n",
    "    mom_2['3_month'] = mom_2['3 Month Total Return'] - spy_rets['3 Month Total Return'].iloc[0]\n",
    "    mom_2['6_month'] = mom_2['6 Month Total Return'] - spy_rets['6 Month Total Return'].iloc[0]\n",
    "    mom_2['12_month'] = mom_2['52 Week Total Return'] - spy_rets['52 Week Total Return'].iloc[0]    \n",
    "    momentum = pd.concat([mom_1, mom_2])\n",
    "    momentum = momentum.filter(['Instrument', 'GICS Sector Name', '3_month', '6_month', '12_month'])\n",
    "    \n",
    "    sectors = list(momentum['GICS Sector Name'].drop_duplicates())\n",
    "    \n",
    "    mom_df= pd.DataFrame()\n",
    "    for s in sectors:\n",
    "        x =  momentum[momentum['GICS Sector Name']==s]\n",
    "        meds = x.median()\n",
    "        x = x.fillna(meds)\n",
    "        mom_df= pd.concat([mom_df, x])\n",
    "        \n",
    "    mom_df[['3_month', '6_month', '12_month']] = win_df(mom_df[['3_month', '6_month', '12_month']])\n",
    "    \n",
    "    momentum = mom_df\n",
    "    min_max = MinMaxScaler()  \n",
    "    momentum[['3_month', '6_month', '12_month']] = min_max.fit_transform(momentum[['3_month', '6_month', '12_month']])\n",
    "    momentum['MOM_Composite'] = momentum.sum(axis = 1)\n",
    "    min_max = MinMaxScaler()    \n",
    "    momentum[['MOM_Composite']]= min_max.fit_transform(momentum[['MOM_Composite']])\n",
    "    mom_comp = momentum['MOM_Composite']\n",
    "    momentum = momentum.drop('MOM_Composite', axis = 1)\n",
    "    \n",
    "    return momentum, mom_comp\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_prep(date):\n",
    "    val, val_comp = valuation_dat(date)\n",
    "    fund, fund_comp = fundamental_dat(date)\n",
    "    mom, mom_comp = momentum_dat(date)\n",
    "    val = val.set_index('Instrument')\n",
    "    fund = fund.set_index('Instrument')\n",
    "    mom = mom.set_index('Instrument')\n",
    "    \n",
    "    fund = fund.filter(['Return On Invested Capital - Mean', 'Long Term Growth - Mean','Spread',])\n",
    "    mom = mom.filter(['3_month', '6_month', '12_month'])\n",
    "    val = val.join(fund, on  = 'Instrument')\n",
    "    val = val.join(mom, on  = 'Instrument')\n",
    "    sectors = list(val['GICS Sector Name'].drop_duplicates())\n",
    "    \n",
    "    features = pd.DataFrame()\n",
    "    for s in sectors:\n",
    "        x = val[val['GICS Sector Name']==s]\n",
    "        meds = x.median()\n",
    "        x = x.fillna(meds)\n",
    "        features = pd.concat([features, x])\n",
    "        \n",
    "    features = features.fillna(0)\n",
    "    features = features.drop('GICS Sector Name', axis = 1)\n",
    "\n",
    "\n",
    "    \n",
    "    #comps = val_comp.to_frame(name = 'Value')\n",
    "    #comps = comps.join(mom_comp)\n",
    "    #comps = comps.join(fund_comp)\n",
    "    #comps['Instrument'] = list(features.index)\n",
    "    \n",
    "    return features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Kmeans(date):\n",
    "    X= feature_prep(date)\n",
    "    clstr = KMeans(n_clusters = 4)\n",
    "    clstr_labs = clstr.fit(X)\n",
    "    labs =  clstr.labels_\n",
    "\n",
    "    \n",
    "    X['Clusters'] = labs\n",
    "    \n",
    "    return X\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def aglom(date):\n",
    "    X= feature_prep(date)\n",
    "    clstr = AgglomerativeClustering(n_clusters = 4)\n",
    "    clstr_labs = clstr.fit(X)\n",
    "    labs =  clstr.labels_\n",
    "\n",
    "    \n",
    "    X['Clusters'] = labs\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_KMtest(dates):\n",
    "    results = dict()\n",
    "    for d in dates:\n",
    "        results[d] = Kmeans(d)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_aglom(dates):\n",
    "    results = dict()\n",
    "    for d in dates:\n",
    "        results[d] = aglom(d)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = dates[3:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_dict = run_KMtest(dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_dict_2 = run_aglom(dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def port_const(mod_dict):\n",
    "    \n",
    "    dates = list(mod_dict.keys())\n",
    "    chain = []\n",
    "    mth = []\n",
    "    sval = 1000000\n",
    "    \n",
    "    for i in range(0,len(dates)):\n",
    "        #try:\n",
    "            #universe = mod_dict[dates[i]]\n",
    "            #eval_df = universe.groupby('Clusters').mean()\n",
    "            #eval_df['AVG_SCORE'] = eval_df.mean(axis = 1)\n",
    "            #selection = eval_df['AVG_SCORE'].idxmax()\n",
    "            #port = universe[universe['Clusters'] == selection]\n",
    "            #rics = list(port.index)\n",
    "            #px_df = ek.get_timeseries(rics, fields = 'CLOSE', start_date = dates[i], end_date = dates[i+1], interval = 'monthly')\n",
    "            #px_df.to_csv('{}_KMEANS_PORT.csv'.format(dates[i]))\n",
    "        px_df = pd.read_csv('{}_KMEANS_PORT.csv'.format(dates[i]), index_col = 'Date')\n",
    "        px_df = px_df.dropna(axis = 1)\n",
    "        px_pershare = list(px_df.iloc[0])\n",
    "        weights = [1/len(px_pershare) for i in px_pershare]\n",
    "        allocs = [i * sval for i in weights]\n",
    "        shares = [i/j for i, j in zip(allocs, px_pershare)]\n",
    "        for d in px_df.index:\n",
    "            v = np.dot(shares, px_df.loc[d])\n",
    "            mth.append(d)\n",
    "            chain.append(v)\n",
    "            sval = v\n",
    "            \n",
    "    returns = pd.DataFrame({'Date':mth, 'Port': chain})\n",
    "    \n",
    "    rsp = pd.read_csv('RSP_DAT.csv', index_col ='Date')\n",
    "    bm_dat = list(rsp['CLOSE'])\n",
    "    returns['BM'] = bm_dat\n",
    "    returns['BM'] = returns['BM']/returns['BM'].iloc[0]\n",
    "    returns['Port'] = returns['Port']/returns['Port'].iloc[0]\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "port_df = port_const(mod_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perf_plot(port_df,port_df_2):\n",
    "    funds = port_df\n",
    "    funds_2 = port_df_2\n",
    "    portfolios = funds.filter(['Date', 'Port', 'BM'])\n",
    "    portfolios_2 = funds_2.filter(['Date', 'Port', 'BM'])\n",
    "    fig_perf = go.Figure()\n",
    "\n",
    "    fig_perf.add_trace(\n",
    "        go.Scatter(\n",
    "        x = portfolios['Date'],\n",
    "        y = portfolios['Port'],\n",
    "        name = 'K_means Stock Selection',\n",
    "        line = {'color': '#00274C'}))\n",
    "\n",
    "\n",
    "\n",
    "    fig_perf.add_trace(\n",
    "        go.Scatter(\n",
    "        x = portfolios['Date'],\n",
    "        y = portfolios_2['Port'],\n",
    "        name = 'Aglomo Stock Selection',\n",
    "        line = {'color': '#FFCB05'}))\n",
    "    \n",
    "    fig_perf.add_trace(\n",
    "        go.Scatter(\n",
    "        x = portfolios['Date'],\n",
    "        y = portfolios['BM'],\n",
    "        name = 'Equal Weight S&P 500',\n",
    "        line = {'color': '#9A3324'}))\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    fig_perf.update_yaxes(tickformat=\".2%\", title = 'Portfolio Return')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    fig_perf.update_layout(legend=dict(orientation='h',yanchor='top',xanchor='center',y=-0.05, x=0.5), paper_bgcolor='white',\n",
    "        plot_bgcolor='white' , height = 800, width = 1200, title_text='Performance', title_x=0.5)\n",
    "    \n",
    "    daily_rp = portfolios['Port'].pct_change().dropna()\n",
    "    daily_6040 = portfolios['BM'].pct_change().dropna()\n",
    "\n",
    "    \n",
    "    mu_a = daily_rp.mean() *252\n",
    "    mu_b = daily_6040.mean() * 252\n",
    "\n",
    "    \n",
    "    sig_a = daily_rp.std() * (252**0.5)\n",
    "    sig_b = daily_6040.std() * (252**0.5)\n",
    "\n",
    "\n",
    "    active_sharpe = (mu_a)/sig_a\n",
    "    bh_sharpe = (mu_b)/sig_b\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    print('RPAR Portfolio Sharpe: {:.2f}'.format(active_sharpe))\n",
    "    print('Bond Sharpe: {:.2f}'.format(bh_sharpe))\n",
    "    print(sig_a, sig_b)\n",
    "\n",
    "    return fig_perf.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_2 = px.scatter_3d(mod_dict[dates[11]], x = '12_month', y ='EBIT_EV', z = 'Spread', color = 'Clusters', hover_name=mod_dict[dates[11]].index, opacity = 0.8)\n",
    "fig_2.update_layout(\n",
    "        paper_bgcolor='white',\n",
    "        plot_bgcolor= 'white',\n",
    "        height = 950,\n",
    "        width = 740,\n",
    "        font = dict(family= 'pt sans narrow'),\n",
    "        title= {'text': \"Kmeans Clusters\", 'font':{'size':25}})\n",
    "    \n",
    "fig_2.update_traces(marker_coloraxis=None, marker=dict(size=4,\n",
    "                              line=dict(width=1)))\n",
    "    \n",
    "    \n",
    "pio.write_html(fig_2, file='kmeansclust.html', auto_open=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_3 = px.scatter_3d(mod_dict_2[dates[11]], x = '12_month', y ='EBIT_EV', z = 'Spread', color = 'Clusters', hover_name=mod_dict_2[dates[11]].index, opacity = 0.8)\n",
    "fig_3.update_layout(\n",
    "        paper_bgcolor='white',\n",
    "        plot_bgcolor= 'white',\n",
    "        height = 950,\n",
    "        width = 740,\n",
    "        font = dict(family= 'pt sans narrow'),\n",
    "        title= {'text': \"Aglom Clusters\", 'font':{'size':25}})\n",
    "    \n",
    "fig_3.update_traces(marker_coloraxis=None, marker=dict(size=4,\n",
    "                              line=dict(width=1)))\n",
    "fig_3.update_layout(font=dict(\n",
    "            size=9,\n",
    "            color=\"Black\"), margin=dict(\n",
    "            l=1,\n",
    "            r=1,\n",
    "            b=1,\n",
    "            t=50,\n",
    "            pad=0,\n",
    "        ))\n",
    "    \n",
    "    \n",
    "fig_3.update_yaxes( automargin=True)\n",
    "fig_3.update_xaxes(automargin=True)  \n",
    "pio.write_html(fig_3, file='aglomclust.html', auto_open=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def port_const_ag(mod_dict):\n",
    "    \n",
    "    dates = list(mod_dict.keys())\n",
    "    chain = []\n",
    "    mth = []\n",
    "    sval = 1000000\n",
    "    \n",
    "    for i in range(0,len(dates)):\n",
    "        #try:\n",
    "            #universe = mod_dict[dates[i]]\n",
    "            #eval_df = universe.groupby('Clusters').mean()\n",
    "            #eval_df['AVG_SCORE'] = eval_df.mean(axis = 1)\n",
    "            #selection = eval_df['AVG_SCORE'].idxmax()\n",
    "            #port = universe[universe['Clusters'] == selection]\n",
    "            #rics = list(port.index)\n",
    "            #px_df = ek.get_timeseries(rics, fields = 'CLOSE', start_date = dates[i], end_date = dates[i+1], interval = 'monthly')\n",
    "            #px_df.to_csv('{}_AGLOM_PORT.csv'.format(dates[i]))\n",
    "        px_df = pd.read_csv('{}_AGLOM_PORT.csv'.format(dates[i]), index_col = 'Date')\n",
    "        px_df = px_df.dropna(axis = 1)\n",
    "        px_pershare = list(px_df.iloc[0])\n",
    "        weights = [1/len(px_pershare) for i in px_pershare]\n",
    "        allocs = [i * sval for i in weights]\n",
    "        shares = [i/j for i, j in zip(allocs, px_pershare)]\n",
    "        for d in px_df.index:\n",
    "            v = np.dot(shares, px_df.loc[d])\n",
    "            mth.append(d)\n",
    "            chain.append(v)\n",
    "            sval = v\n",
    "        #except:\n",
    "         #   universe = mod_dict[dates[i]]\n",
    "         #   eval_df = universe.groupby('Clusters').mean()\n",
    "         #   eval_df['AVG_SCORE'] = eval_df.mean(axis = 1)\n",
    "         #   selection = eval_df['AVG_SCORE'].idxmax()\n",
    "         #   port = universe[universe['Clusters'] == selection]\n",
    "         #   rics = list(port.index)\n",
    "         #   px_df = ek.get_timeseries(rics, fields = 'CLOSE', start_date = dates[i], interval = 'monthly')\n",
    "            #px_df.to_csv('{}_AGLOM_PORT.csv'.format(dates[i]))\n",
    "         #   px_df = pd.read_csv('{}_AGLOM_PORT.csv'.format(dates[i]), index_col = 'Date')\n",
    "         #   px_df = px_df.dropna(axis = 1)\n",
    "         #   px_pershare = list(px_df.iloc[0])\n",
    "         #   weights = [1/len(px_pershare) for i in px_pershare]\n",
    "         #   allocs = [i * sval for i in weights]\n",
    "         #   shares = [i/j for i, j in zip(allocs, px_pershare)]\n",
    "         #   for d in px_df.index:\n",
    "         #       v = np.dot(shares, px_df.loc[d])\n",
    "         #       mth.append(d)\n",
    "         #       chain.append(v)\n",
    "         #       sval = v           \n",
    "            \n",
    "            \n",
    "    returns = pd.DataFrame({'Date':mth, 'Port': chain})\n",
    "    \n",
    "    rsp = pd.read_csv('RSP_DAT.csv', index_col ='Date')\n",
    "    bm_dat = list(rsp['CLOSE'])\n",
    "    returns['BM'] = bm_dat\n",
    "    returns['BM'] = returns['BM']/returns['BM'].iloc[0]\n",
    "    returns['Port'] = returns['Port']/returns['Port'].iloc[0]\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "port_df_2 = port_const_ag(mod_dict_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_plot(port_df, port_df_2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aglo_df = mod_dict_2[dates[0]].groupby('Clusters').mean()\n",
    "aglo_df['AVG_SCORE'] = aglo_df.mean(axis = 1)\n",
    "aglo_df.sort_values('AVG_SCORE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km_df = mod_dict[dates[0]].groupby('Clusters').mean()\n",
    "km_df['AVG_SCORE'] = km_df.mean(axis = 1)\n",
    "km_df.sort_values('AVG_SCORE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
